{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Collaborative Filtering</h1>\n",
    "\n",
    "# 0. Matrix Factorization (~SVD)\n",
    "\n",
    "[1] Matrix factorization techniques for recommender systems.\n",
    "\n",
    "Note: Not the same with pure SVD in math which decomposite matrix to three sub-matrices.\n",
    "\n",
    ">  $\\min_{q^*, p^*} \\sum_{u, i} (r_{u, i} - q_i^T p_u)^2 \n",
    "+ \\lambda(||q_i||^2 + ||p_u||^2)$\n",
    "\n",
    "## 0.0 SGD\n",
    "\n",
    "> $e_{u, i} = r_{u, i} - q_i^T p_u$\n",
    "\n",
    "> $q_i \\leftarrow q_i - \\eta (- e_{u, i} p_u + \\lambda * 2 * q_i )$\n",
    "\n",
    "> $p_u \\leftarrow p_u - \\eta (- e_{u, i} q_i + \\lambda * 2 * p_u )$\n",
    "\n",
    "## 0.1 ALS\n",
    "In each iteration, fix $p_u$ and $q_i$ seperately.\n",
    "\n",
    "Thus the optimization object becomes covex, \n",
    "the problem can be solved using least square methods.\n",
    "\n",
    "ALS is favorable in at least two cases,\n",
    "\n",
    "+ Parallelization\n",
    "\n",
    "+ Implicit Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 1. Implicit Datasets\n",
    "\n",
    "[1] Collaborative Filtering for Implicit Feedback Datasets\n",
    "\n",
    "Let $p_{u, i} = 1$ if $r_{u, i} > 0$ otherwise 0.\n",
    "\n",
    "> $\\min_{x^*, y^*} c_{u, i}(p_{u, i} - x_u^T y_i)^2 \n",
    "+ \\lambda \\left(\\sum_u||x_u||^2 + \\sum_i ||y_i||^2 \\right)$\n",
    "\n",
    "$c_{u, i}$ is the confidence of implicit record, a plausible choice is,\n",
    "\n",
    "> $c_{u, i} = 1 + \\alpha r_{u, i}$\n",
    "\n",
    "## 1.0 Least Square Methods\n",
    "\n",
    "> $W^* = min_{W} \\frac{1}{2} ||y - X^T W||^2 \n",
    "= \\frac{1}{2} (y - X^T W)^T (y - X^T W)$\n",
    "\n",
    "To obtain the optimal $W$, compute the gradient, let it be zero, \n",
    "\n",
    "> $\\frac{\\partial{L}}{\\partial{W}} = \n",
    "\\frac{1}{2} \\frac{\\partial{(y - X^T W)^T (y - X^T W)}}{\\partial{W}}$\n",
    "\n",
    "Recall that in the denominator way,  \n",
    "\n",
    "> $\\vec{u} = \\vec{u}(x)$\n",
    "\n",
    "> $\\vec{v} = \\vec{v}(x)$\n",
    "\n",
    "> $\\frac{\\partial{\\vec{u}^T \\vec{v}}}{\\vec{x}} = \n",
    "\\frac{\\partial{\\vec{u}}}{\\partial{\\vec{x}}} \\vec{v} + \n",
    "\\frac{\\partial{\\vec{v}}}{\\partial{\\vec{x}}} \\vec{u}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let $\\vec{u} = y - X^T W$, $\\vec{v} = y - X^T W$,\n",
    "\n",
    "> $\\frac{\\partial{(y - X^T W)^T (y - X^T W)}}{\\partial{W}} = \n",
    "\\frac{\\partial{(y - X^T W)}}{\\partial{W}} (y - X^T W) + \n",
    "\\frac{\\partial{(y - X^T W)}}{\\partial{W}} (y - X^T W)$\n",
    "\n",
    "> $ = -2 X (y - X^T W) = 0$\n",
    "\n",
    "$\\Longrightarrow$\n",
    "\n",
    "> $X y = X X^T W$\n",
    "\n",
    "If $XX^T$ is inversible, \n",
    "\n",
    "> $W^* = (XX^T)^{-1} X y$\n",
    "\n",
    "## 1.1 Alternative Least Square\n",
    "\n",
    "Fix $x_u$ and $y_i$ alternativly, \n",
    "\n",
    "thus get a convex function of $y_i$ and $x_u$ separatly, \n",
    "\n",
    "which can be solved using least square methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Factorization Machines\n",
    "\n",
    "[1] Factorization Machines.\n",
    "\n",
    "## 2.0 2-way factorization machines\n",
    "\n",
    "> $\\hat{y} = w_0 + \\sum_{i=1}^n w_i x_i + \\sum_{i=1}^n \\sum_{j=i+1}^n <v_i, v_j> x_i x_j$\n",
    "\n",
    "parameters to be learned, \n",
    "\n",
    "> $w_0 \\in \\mathbb{R}$\n",
    "\n",
    "> $\\vec{w} \\in \\mathbb{R}^n$\n",
    "\n",
    "> $V \\in \\mathbb{R} ^ {n \\times K}, <v_i, v_j> = \\sum_{k=1}^K v_{i, k} v_{j, k}$\n",
    "\n",
    "<b>Note:</b> If $W$ is a positive definite matrix, there exsits a matrix $V$ such that, \n",
    "\n",
    "> $W = V V^T$ \n",
    "\n",
    "when $K$ is sufficiently large. In practice, $K$ is offen not so large especially for sparse data.\n",
    "\n",
    "The original time complexity is $O(Kn^2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that $V$ is <b>symmetric</b> matrix, the pairwise interaction between $x_i$ and $x_j$ ($i < j$) is an upper traingular matrix of $V$ <b>without</b> the diagonal elements.\n",
    "\n",
    "Hence, \n",
    "\n",
    "> $\\sum_{i=1}^n \\sum_{j=i+1}^n <v_i, v_j> x_i x_j$\n",
    "\n",
    "> $ = \\frac{1}{2} \\left \\{ \n",
    "\\sum_{i=1}^n \\sum_{j=1}^n <v_i, v_j> x_i x_j  - \\sum_{i=1}^n <v_i, v_i> x_i, x_i\n",
    "\\right \\}$\n",
    "\n",
    "> $= \\frac{1}{2} \\left \\{ \n",
    "\\sum_{i=1}^n \\sum_{j=1}^n \\sum_{k=1}^K v_{i,k} v_{j,k} x_i x_j - \n",
    "\\sum_{i=1}^n \\sum_{k=1}^{K} v_{i,k} v_{i,k} x_i x_i\n",
    "\\right \\}$\n",
    "\n",
    "> $= \\frac{1}{2} \\sum_{k=1}^K \\left \\{ \n",
    "\\sum_{i=1}^n \\sum_{j=1}^n v_{i,k} v_{j,k} x_i x_j - \n",
    "\\sum_{i=1}^n v_{i,k} v_{i,k} x_i x_i\n",
    "\\right \\}$\n",
    "\n",
    "> $= \\frac{1}{2} \\sum_{k=1}^K \\left \\{ \n",
    "\\left( \\sum_{i=1}^n v_{i,k} x_i \\right) \n",
    "\\left( \\sum_{j=1}^n v_{j,k} x_j \\right) - \n",
    "\\sum_{i=1}^n (v_{i,k} x_i)^2\n",
    "\\right \\}$\n",
    "\n",
    "> $= \\frac{1}{2} \\sum_{k=1}^K \\left \\{ \n",
    "\\left( \\sum_{i=1}^n v_{i,k} x_i \\right)^2 - \n",
    "\\sum_{i=1}^n (v_{i,k} x_i)^2\n",
    "\\right \\}$\n",
    "\n",
    "The time comlexity reduces to $O(K n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 D-way Factorization Machines\n",
    "\n",
    "> $\\hat{y} = w_0 + \\sum_{i=1}^n w_i x_i +\n",
    "\\sum_{d=2}^D\\sum_{i_1=1}^n \\sum_{i_2=i_1+1}^n ... \\sum_{i_D = i_{D-1}+1}^n\n",
    "\\left( \\prod_{j=1}^d x_{i_j} \\right)\n",
    "\\left( \\sum_{k=1}^K  \\prod_{j=1}^d v_{i_j, k}^{(d)}  \\right)\n",
    "$\n",
    "\n",
    "> $V^{(d)} \\in \\mathbb{R}^{n \\times k_d}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Field-aware Factorization Machines \n",
    "\n",
    "[1] Field-aware Factorization Machines for CTR Prediction\n",
    "\n",
    "> $\\hat{y} = w_0 + \\sum_{i=1}^n w_i x_i + \\sum_{i=1}^n \\sum_{j=i+1}^n \n",
    "<v_{i, f_j}, v_{j, f_i}> x_i x_j$\n",
    "\n",
    "$f_j$ is the field of $x_j$, $f_i$ is the field of $x_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
